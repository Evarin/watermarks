{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from pathlib import Path\n",
    "from torchvision import transforms as T\n",
    "from PIL import Image, ImageOps\n",
    "from tqdm.notebook import tqdm\n",
    "import json\n",
    "import shutil\n",
    "from sqlalchemy import create_engine, text\n",
    "from PIL import Image, ImageOps\n",
    "\n",
    "device = torch.device(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automatically detect and crop\n",
    "\n",
    "This part of the script run the detection model on all the images of a source directory, and crop the images to the detected bounding box. The cropped images are saved in a destination directory.\n",
    "\n",
    "If no watermark is detected, the original image is copied to a special subdirectory \"notfound\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DETECTION_MODEL = \"../detection.pth\"\n",
    "DATA_DIR = Path(\"../data\")\n",
    "SOURCE_DIR = DATA_DIR / \"xishen\" / \"A_classification\"\n",
    "TARGET_DIR = DATA_DIR / \"detected\" # caution: will be wiped out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = torch.load(DETECTION_MODEL).to(device).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TARGET_DIR.exists():\n",
    "    shutil.rmtree(TARGET_DIR)\n",
    "\n",
    "TARGET_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "dirs = [d for d in SOURCE_DIR.iterdir() if d.is_dir()]\n",
    "nobox = []\n",
    "allboxes = {}\n",
    "\n",
    "for d in tqdm(dirs):\n",
    "    if d.name == \"notfound\": continue\n",
    "\n",
    "    ddir = TARGET_DIR / d.name\n",
    "    ddir.mkdir(parents=True, exist_ok=True)\n",
    "    files = [f for f in d.glob(\"**/*\") if f.is_file() and f.suffix.lower() in [\".jpg\", \".jpeg\", \".png\"]]\n",
    "\n",
    "    for f in tqdm(files):\n",
    "        n = int(f.name.split(\".\")[0].split(\"-\")[-1])\n",
    "\n",
    "        img = Image.open(f)\n",
    "        img0 = ImageOps.exif_transpose(img)\n",
    "        img = T.ToTensor()(img)\n",
    "        img = img.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            out = m([img])[0]\n",
    "            boxes = out[\"boxes\"]\n",
    "            scores = out[\"scores\"]\n",
    "        boxes = boxes[scores > 0.5]\n",
    "        \n",
    "        if len(boxes) == 0:\n",
    "            print(f\"no box found for {f}\")\n",
    "            nobox.append(f)\n",
    "            continue\n",
    "        \n",
    "        crops = []\n",
    "        for k, box in enumerate(boxes):\n",
    "            x0, y0, x1, y1 = [float(f) for f in box]\n",
    "            # rescale to original size\n",
    "            sx, sy = img.shape[-1], img.shape[-2]\n",
    "            x0, y0, x1, y1 = x0 / sx, y0 / sy, x1 / sx, y1 / sy\n",
    "            oarea = (x1 - x0) * (y1 - y0)\n",
    "            if oarea > 0.3 or (x1-x0) > 0.8 or (y1-y0) > 0.8:\n",
    "                print(f\"Box {k} too large {oarea:0.2f}\", f)\n",
    "                continue\n",
    "\n",
    "            # compute intersections with previous crops\n",
    "            ignore = False\n",
    "            for crop in crops:\n",
    "                x0_, y0_, x1_, y1_ = crop[\"box\"]\n",
    "                intersect = (max(x0, x0_), max(y0, y0_), min(x1, x1_), min(y1, y1_))\n",
    "                if intersect[2] < intersect[0] or intersect[3] < intersect[1]:\n",
    "                    continue\n",
    "                area = (intersect[2] - intersect[0]) * (intersect[3] - intersect[1])\n",
    "                if area / oarea > 0.5:\n",
    "                    ignore = True\n",
    "                    print(f\"Ignoring box {k} overlapping box {crop['k']} by {area/oarea:0.2f}\", f)\n",
    "                    break\n",
    "            \n",
    "            if ignore: continue\n",
    "            crops.append({\"k\": k, \"box\": (x0, y0, x1, y1)})\n",
    "            \n",
    "            sx, sy = img0.size\n",
    "            x0, y0, x1, y1 = int(x0 * sx), int(y0 * sy), int(x1 * sx), int(y1 * sy)\n",
    "            # convert to cx cy w h\n",
    "            cx, cy, w, h = (x0 + x1) / 2, (y0 + y1) / 2, x1 - x0, y1 - y0\n",
    "            # add 15% padding and square\n",
    "            sz = max(w, h) * 1.2\n",
    "            # crop\n",
    "            x0, y0, x1, y1 = int(cx - sz / 2), int(cy - sz / 2), int(cx + sz / 2), int(cy + sz / 2)\n",
    "            score = int(scores[k].item()*100)\n",
    "            crop = img0.crop((x0, y0, x1, y1))\n",
    "            crop.thumbnail((640, 640))\n",
    "            tfile = ddir / f.parent.relative_to(d) / f\"{f.name.split('.')[0]}+{k}({score:03d}).jpg\"\n",
    "            tfile.parent.mkdir(parents=True, exist_ok=True)\n",
    "            crop.save(tfile, quality=85)\n",
    "\n",
    "        allboxes[f\"{f.parent.name}/{f.name}\"] = crops\n",
    "\n",
    "with open(TARGET_DIR / \"boxes.json\", \"w\") as target:\n",
    "    json.dump(allboxes, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for f in nobox:\n",
    "    # copy to notfound, so they can be processed manually\n",
    "    ddir = TARGET_DIR / \"notfound\" / f.parent.name\n",
    "    ddir.mkdir(parents=True, exist_ok=True)\n",
    "    shutil.copyfile(f, ddir / f.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collect manually annotated watermarks\n",
    "\n",
    "Use the annotator in this repository to annotate the watermarks in the folder \"notfound\", then run this part of the script to collect the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ANNOTATOR_DB_FILE = \"../detection/annotator/db.sqlite3\"\n",
    "PREFIX = (TARGET_DIR / \"notfound\").relative_to(DATA_DIR)\n",
    "\n",
    "engine = create_engine(f'sqlite:///{ANNOTATOR_DB_FILE}', echo=False)\n",
    "\n",
    "with engine.connect() as connection:\n",
    "    annotations = connection.execute(text(f'SELECT * FROM ANNOTATIONS WHERE image LIKE(\"/{PREFIX}/%\")')).fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for annot in annotations:\n",
    "    f = DATA_DIR / annot.image\n",
    "    n = int(f.name.split(\".\")[0].split(\"-\")[-1])\n",
    "\n",
    "    img = Image.open(f)\n",
    "    img0 = ImageOps.exif_transpose(img)\n",
    "    boxes = [(annot.x0, annot.y0, annot.x1, annot.y1)]\n",
    "    ddir = TARGET_DIR / f.parent.name\n",
    "\n",
    "    for k, box in enumerate(boxes):\n",
    "        x0, y0, x1, y1 = [float(f) for f in box]\n",
    "        # rescale to original size\n",
    "        crops.append({\"k\": k, \"box\": (x0, y0, x1, y1), \"manual\": True})\n",
    "        sx, sy = img0.size\n",
    "        x0, y0, x1, y1 = int(x0 * sx), int(y0 * sy), int(x1 * sx), int(y1 * sy)\n",
    "        # convert to cx cy w h\n",
    "        cx, cy, w, h = (x0 + x1) / 2, (y0 + y1) / 2, x1 - x0, y1 - y0\n",
    "        # add 15% padding and square\n",
    "        sz = max(w, h) * 1.2\n",
    "        # crop\n",
    "        x0, y0, x1, y1 = int(cx - sz / 2), int(cy - sz / 2), int(cx + sz / 2), int(cy + sz / 2)\n",
    "        score = int(scores[k].item()*100)\n",
    "        crop = img0.crop((x0, y0, x1, y1))\n",
    "        crop.thumbnail((640, 640))\n",
    "\n",
    "        crop.save(ddir / f\"{f.name.split('.')[0]}+{k}(100).jpg\", quality=85)\n",
    "\n",
    "    allboxes[f\"{f.parent.name}/{f.name}\"] = crops\n",
    "\n",
    "with open(TARGET_DIR / \"added_boxes.json\", \"w\") as target:\n",
    "    json.dump(allboxes, target)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
